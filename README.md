# Reinforcement Learning for Blackjack

A comparative study of **Q-learning**, **SARSA**, and **Monte Carlo** to train agents to play **Blackjack** through self-play. Agents learn when to **HIT** or **STAND** using different learning methods and exploration strategies.

---

## Algorithms Implemented

### 1. Q-Learning (Off-policy TD Control)
- Learns by estimating the **maximum future reward**.
- Supports multiple Œµ-greedy exploration strategies:
  - `Œµ = 0.1` (constant)
  - `Œµ = 1/k` (decay)
  - `Œµ = e^(-k/1000)`, `Œµ = e^(-k/10000)` (exponential decay)

### 2. SARSA (On-policy TD Control)
- Learns using the **actual next action** taken.
- Trained with the same four Œµ strategies as Q-learning.

### 3. Monte Carlo (First-Visit)
- Learns by **completing entire episodes** before updating.
- Two approaches:
  - **Exploring Starts (ES)** ‚Äì Forces exploration from all states
  - **Non-Exploring Starts (NES)** ‚Äì Uses Œµ-greedy with decay

---

## Objectives

- Learn optimal HIT/STAND strategy for Blackjack.
- Compare different RL methods and exploration strategies.
- Evaluate using:
  - Win/Loss/Draw statistics
  - State-action space coverage
  - Strategy tables
  - Dealer advantage

---

## Visualizations

Each method includes:

- **Win/Loss/Draw graphs**
- **State-action pair bar plots**
- **Strategy tables** (with/without Ace as 11)
- **Dealer Advantage** comparison across methods

---

## üìÅ Project Structure

```bash
Blackjack-RL
‚îú‚îÄ‚îÄ Qlearningnew.py          # Q-learning implementation
‚îú‚îÄ‚îÄ Sarsanew.py              # SARSA implementation
‚îú‚îÄ‚îÄ main.py                  # Monte Carlo + evaluation/visualizations
‚îî‚îÄ‚îÄ README.md                # You're here!
